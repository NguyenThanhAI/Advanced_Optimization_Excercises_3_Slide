%% This Beamer template is based on the one found here: https://github.com/sanhacheong/stanford-beamer-presentation, and edited to be used for Stanford ARM Lab

\documentclass[10pt]{beamer}
%\mode<presentation>{}

\usepackage{media9}
\usepackage{amssymb,amsmath,amsthm,enumerate}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage[parfill]{parskip}
\usepackage[utf8]{vietnam}
\usepackage{graphicx,animate}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{amsfonts,amscd}
\usepackage[]{units}
\usepackage{listings}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{physics}
\usepackage{movie15}
% Enable colored hyperlinks
\hypersetup{colorlinks=true}

% The following three lines are for crossmarks & checkmarks
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Numbered captions of tables, pictures, etc.
\setbeamertemplate{caption}[numbered]
\usepackage{media9} 
%\usepackage[superscript,biblabel]{cite}
%\usepackage{algorithmic}
%\usepackage{algorithm2e}
\usepackage{algpseudocode}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{caption}
%\usepackage{xcolor}
\usepackage{array}
%\renewcommand{\thealgocf}{}

\usepackage[natbib,backend=biber,style=ieee, sorting=ynt]{biblatex}
\bibliography{ref.bib}

\usepackage[acronym]{glossaries}

\usepackage{graphicx}
\graphicspath{{./figures}}
\usepackage{hyperref}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}

\newcommand{\empy}[1]{{\color{darkorange}\emph{#1}}}
\newcommand{\empr}[1]{{\color{cardinalred}\emph{#1}}}
\newcommand{\examplebox}[2]{
\begin{tcolorbox}[colframe=darkcardinal,colback=boxgray,title=#1]
#2
\end{tcolorbox}}

\usetheme{Stanford} 
\input{./style_files_stanford/my_beamer_defs.sty}
\logo{\includegraphics[height=0.5in]{logos/HUS-name.jpg}}

\makeatletter
\let\@@magyar@captionfix\relax
\makeatother

\title[Tối ưu hóa nâng cao]{Tối ưu hóa nâng cao: Bài tập 3}

\begin{document}

\nocite{*}

\author[Nguyễn Chí Thanh - Nguyễn Đức Thịnh]{
	\begin{tabular}{c} 
	\Large
	Nguyễn Chí Thanh \\
    \footnotesize \href{mailto:nguyenchithanh\_sdh21@hus.edu.vn}{nguyenchithanh\_sdh21@hus.edu.vn} \\
	\Large
    Nguyễn Đức Thịnh \\
     \footnotesize \href{mailto:nguyenducthinh\_sdh21@hus.edu.vn}{nguyenducthinh\_sdh21@hus.edu.vn}
\end{tabular}
\vspace{-4ex}}

\institute{
	\vskip 5pt
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.5\textwidth}
			\centering
			\includegraphics[height=0.75in]{logos/HUS-logo.jpg}
		\end{subfigure}%
		~ 
		\begin{subfigure}[t]{0.5\textwidth}
			\centering
			\includegraphics[height=0.75in]{logos/MIM-logo.png}
		\end{subfigure}
	\end{figure}
	\vskip 10pt	
	Đại học Quốc Gia Hà Nội \\
	Trường đại học Khoa học tự nhiên\\
	Khoa Toán - Cơ - Tin học
	\vskip 3pt
}

\begin{noheadline}
\begin{frame} \maketitle \end{frame}
\end{noheadline}
    
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{itemize subitem}[circle]

\begin{frame}{Nội dung}

    \begin{enumerate}
        \item So sánh ba thuật toán
        \item So sánh các thuật toán
        \item Kết luận
    \end{enumerate}
    
\end{frame}

\begin{frame}[allowframebreaks]{So sánh ba thuật toán}
	\begin{itemize}
		\item Hàm loss của Linear Regression:
		\begin{equation*}
			f(w) = \lVert X w - y \rVert_2^2
		\end{equation*}
		\item Ta đặt kích thước các ma trận $X \in \mathbb{R}^{m \times n}, y \in \mathbb{R}^m, w \in \mathbb{R}^n$, ta có $\mathrm{dom}(f)=\mathbb{R}^{n}$ là tập lồi
		\begin{equation*}
			\begin{aligned}
				f(w) &= (Xw - y)^T (Xw - y) \\
				&=w^T X^T X w - w^TX^Ty - y^T Xw + y^T y
			\end{aligned}
		\end{equation*}
		\item Do $w^TX^Ty$ và $y^TXw$ là hai số vô hướng và $(w^TX^Ty)^T=y^TXw$ nên $w^TX^Ty=y^TXw$ (chuyển vị của một số vô hướng bằng chính nó) nên:
		\begin{equation*}
			\begin{aligned}
				f(w) &= w^T X^T X w - w^TX^Ty - y^T Xw + y^T y\\
				&=w^T X^T X w-2w^TX^Ty+y^Ty
			\end{aligned}
		\end{equation*}
		\item Ta tính Gradient của $f(w)$ theo $w$:
		\begin{equation*}
			\nabla f(w) =\Big\lbrack X^TX + (X^TX)^T \Big\rbrack w - 2X^Ty = 2X^T X w - 2 X^T y
		\end{equation*}
		\item Ta tính ma trận Hessian của $f(w)$ theo $w$:
		\begin{equation*}
			\nabla^2 f(w) = 2X^T X
		\end{equation*}
		\item Ta xét:
		\begin{equation*}
			p^T \nabla^2 f(w) p = 2 p^T X^T X p = 2 (Xp)^TXp \geq 0 \thickspace\forall \thickspace p \in \mathbb{R}^{n}
		\end{equation*}
		nên:
		\begin{equation*}
			\nabla^2 f(w) \succeq 0 \thickspace \forall w \in \mathbb{R}^{n}
		\end{equation*}
		\item Vì $\mathrm{dom}(f)$ là tập lồi và $\nabla^2 f(w) \succeq 0$ nên hàm $f(w)$ là một hàm lồi.
	\end{itemize}
	Hàm loss được sử dụng trong lập trình:
	\begin{equation*}
		f(w) = \dfrac{1}{n}\sum_{i=1}^{n}\Big( x_i^T w - y_i \Big)^2
	\end{equation*}
	với $(x_i, y_i) \in \mathbb{R}^{n+1}, i=1, 2, \dots, n$ là cặp dữ liệu đầu vào và nhãn tương ứng với $w \in \mathbb{R}^n$
	
	\framebreak
	Nghiệm tối ưu (khi các cột của ma trận $X$ độc lập tuyến tính):

	\begin{equation*}
		w^{*} = \Big(X^T X \Big)^{-1}X^T y
	\end{equation*}

	Với dữ liệu tập train:

	\begin{equation*}
		w_{train}^{*} = 
	\end{equation*}

	và giá trị hàm loss tương ứng:

	\begin{equation*}
		f(w_{train}^{*}, \text{ data }_{\text{train}}) \approx 0.04804
	\end{equation*}

	\begin{equation*}
		f(w_{train}^{*}, \text{ data }_{\text{val}}) \approx 0.04959
	\end{equation*}

	\framebreak
	
	Với dữ liệu tập validation:

	\begin{equation*}
		w_{val}^{*} = 
	\end{equation*}

	và giá trị hàm loss tương ứng:

	\begin{equation*}
		f(w_{val}^{*}, \text{ data }_{\text{train}}) \approx 0.06977
	\end{equation*}

	\begin{equation*}
		f(w_{val}^{*}, \text{ data }_{\text{val}}) \approx 0.06900
	\end{equation*}
\end{frame}

\begin{frame}[allowframebreaks]{So sánh các thuật toán}
	Các thuật toán được so sánh:
	\begin{multicols}{3}
	\begin{enumerate}
		\item Gradient Descent
  		\item Newton
    	\item Accelerated
     	\item BFGS
      	\item DFP
        \item Momentum
        \item Adagrad
        \item Adadelta
        \item RMSProp
        \item Adam
        \item Adamax
        \item AMSGrad
        \item Nadam
        \item AdaBelief
        \item RAdam
        \item Avagrad    
	\end{enumerate}
	\end{multicols}
	Các step size: $10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 2, 5, 10$
	
	Các kiểu chọn step size từng bước: Fixed, Backtracking, Inverse decay, Warmup, Triangular Cyclic

	%\begin{figure}
	%	\centering
	%	\begin{subfigure}[b]{0.45\textwidth}
	%		\centering
	%		\includegraphics[width=\textwidth]{Thanh/warmup_lr.png}
	%		\caption{$y=x$}
	%	\end{subfigure}
	%	\hfill
	%	\begin{subfigure}[b]{0.45\textwidth}
	%		\centering
	%		\includegraphics[width=\textwidth]{Thanh/triangular_cyclic.png}
	%		\caption{$y=3sinx$}
	%	\end{subfigure}
	%\end{figure}
	\begin{figure}[!htp]
		% Maximum length
		\subfloat[Warmup step size]{\label{fig1:a}\includegraphics[width=0.5\linewidth]{Thanh/warmup_lr.png}}\hfill
		\subfloat[Triangular cyclic step size]{\label{fig1:b}\includegraphics[width=0.5\linewidth]{Thanh/triangular_cyclic.png}}%
	  \end{figure}

	Mỗi nhóm gồm thuật toán tối ưu, step size, kiểu chọn step size sẽ tạo thành một tổ hợp.
	Một tổ hợp được dùng để train 5000 bước
\end{frame}

\begin{frame}[allowframebreaks]{Tài liệu tham khảo}
    \printbibliography
\end{frame}

\begin{frame}[allowframebreaks]{Phụ lục}
	\begin{enumerate}
		\item  Gradient Descent
		\begin{equation*}
			\begin{cases}g_t = \nabla f(w_{t-1}) \\ w_t = w_{t-1} - \alpha g_t\end{cases}
		\end{equation*}
		\item Momentum
		\begin{equation*}
			\begin{cases}m_0 = 0 \\ g_t = \nabla f(w_{t-1}) 
				\\ m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t\\ w_t = w_{t-1} - \alpha m_t\end{cases}
		\end{equation*}
		\item Adagrad
		\begin{equation*}
			\begin{cases}v_0 = 0\\ g_t = \nabla f(w_{t-1}) \\ v_t = v_{t-1} + g_t^2 \\ w_t = w_{t-1} - \alpha \dfrac{g_t}{\sqrt{v_t} + \epsilon} \end{cases}
		\end{equation*}
		\item Adadelta
		\begin{equation*}
			\begin{cases}v_0 = 0\\d_0 = 0 \\ g_t = \nabla f(w_{t-1}) \\ v_t = \beta v_{t-1} + (1-\beta)g_t^2 \\ \Delta w = - \alpha \dfrac{\sqrt{d_{t-1} + \epsilon}g_t}{\sqrt{v_t + \epsilon}} \\ w_t = w_{t-1} + \Delta w \\ d_t = \beta d_{t-1} + (1 - \beta) \Delta w^2 \end{cases}
		\end{equation*}
		\item RMSProp
		\begin{equation*}
			\begin{cases}v_0 = 0\\ g_t = \nabla f(w_{t-1}) \\ v_t = \beta v_{t-1} + (1-\beta)g_t^2 \\ w_t = w_{t-1} - \alpha \dfrac{g_t}{\sqrt{v_t} + \epsilon} \end{cases}
		\end{equation*}
		\item Adam
		\begin{equation*}
			\begin{cases}m_0 = 0\\ v_0 = 0\\ g_t = \nabla f(w_{t-1}) \\ m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\ v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\
				\hat{m}_t = \dfrac{m_t}{1 - \beta_1^t} \\ \hat{v}_t = \dfrac{v_t}{1 - \beta_2^t} \\ w_t = w_{t-1} - \alpha \dfrac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} \end{cases}
		\end{equation*}
		\item Adamax
		\begin{equation*}
			\begin{cases}m_0 = 0\\ u_0 = 0 \\ g_t = \nabla f(w_{t-1}) \\ m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\ u_t = \max(\beta_2 u_{t-1}, \lVert g_t \rVert_{\infty}) \\ w_t = w_{t-1} - \alpha \dfrac{m_t}{(1 - \beta_1^t)u_t + \epsilon}\end{cases}
		\end{equation*}
		\item AMSGrad
		\begin{equation*}
			\begin{cases}m_0 = 0\\ v_0 = 0\\ g_t = \nabla f(w_{t-1}) \\ m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\ v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\ \hat{v}_t = \max(\hat{v}_{t-1}, v_t) \\ w_t = w_{t-1} - \alpha \dfrac{m_t}{\sqrt{\hat{v}_t} + \epsilon} \end{cases}
		\end{equation*}
		\item Nadam
		\begin{equation*}
			\begin{cases}m_0 = 0\\ v_0 = 0\\ g_t = \nabla f(w_{t-1}) \\ m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\ v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\
				\hat{m}_t = \dfrac{m_t}{1 - \beta_1^t} \\ \hat{v}_t = \dfrac{v_t}{1 - \beta_2^t} \\ w_t = w_{t-1} -  \dfrac{\alpha}{\sqrt{\hat{v}_t} + \epsilon}\Bigg(\beta_1\hat{m}_t + \dfrac{1-\beta_1}{1-\beta_1^t}g_t\Bigg) \end{cases}
		\end{equation*}
		\item AdaBelief
		\begin{equation*}
			\begin{cases}m_0 = 0\\ s_0 = 0\\ g_t = \nabla f(w_{t-1}) \\ m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\ s_t = \beta_2 s_{t-1} + (1-\beta_2)(g_t - m_t)^2 + \epsilon\\
				\hat{m}_t = \dfrac{m_t}{1 - \beta_1^t} \\ \hat{s}_t = \dfrac{s_t}{1 - \beta_2^t} \\ w_t = w_{t-1} - \alpha \dfrac{\hat{m}_t}{\sqrt{\hat{s}_t} + \epsilon} \end{cases}
		\end{equation*}
		\item Accelerated Gradient
		\begin{equation*}
			\begin{cases}v_t = w_{t-1} + \dfrac{t - 1}{t + 2} \big(w_{t-1} - w_{t-2} \big) \\
			w_{t+1} = v_t - \alpha \nabla f(v_t) \end{cases}
		\end{equation*}
		\item RAdam
		\begingroup
			\scalebox{0.8}{
				\begin{minipage}{1.2 \linewidth}
					\begin{algorithm}[H]
						\caption{Thuật toán RAdam}
						\hspace*{\algorithmicindent} \textbf{Input:} {độ dài bước $\lbrace \alpha_t \rbrace_{t=1}^{T}$, các hệ số $\beta_1, \beta_2$, $w_0$ khởi tạo, hàm mục tiêu $f(w)$} \\
						\hspace*{\algorithmicindent} \textbf{Output:} {$w$ đã được học}
						\begin{algorithmic}[1]
							\State{$m_0 \leftarrow 0$}
							\State{$v_0 \leftarrow 0$}
							\State{$\rho_{\infty} \leftarrow 2/(1-\beta_2)-1$}
							\For{$t=1$ to $T$}
								\State{$g_t \leftarrow \nabla f(w_{t-1})$}
								\State{$m_t \leftarrow \beta_1 m_{t-1} + (1 - \beta_1)g_t$}
								\State{$v_t \leftarrow \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$}
								\State{$\widehat{m}_t \leftarrow m_t / (1 - \beta_1^t)$}
								\State{$\rho_t \leftarrow \rho_{\infty} - 2t\beta_2^t / (1 - \beta_2^t)$}
								\If {$\rho_t < 4$}
									\State{$\widehat{v}_t \leftarrow \sqrt{v_t / (1 - \beta_2^t)}$}
									\State{$r_t \leftarrow \sqrt{\frac{(\rho_t - 4)(\rho_t - 2)\rho_{\infty}}{(\rho_{\infty} - 4)(\rho_{\infty} - 2)\rho_t}}$}
									\State{$w_t \leftarrow w_{t-1} - \alpha r_t \widehat{m}_t / (\widehat{v}_t + \epsilon)$}
								\Else
									\State{$w_t \leftarrow w_{t-1} - \alpha \widehat{m}_t$}
								\EndIf
							\EndFor
							\State \Return $w_T$
						\end{algorithmic}
					\end{algorithm}
				\end{minipage}
			}
		\endgroup
		\item Avagrad
		\begin{equation*}
			\begin{cases} w_0 \in \mathbb{R}^d \\ m_0 = 0 \\ v_0 = 0 \\
			g_t = \nabla f(w_{t-1}) \\
			m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\
			\eta_t = \dfrac{1}{\sqrt{v_{t-1}} + \epsilon} \\
			w_t = w_{t-1} - \alpha \dfrac{\eta_t}{\lVert \eta_t / \sqrt{d} \rVert_2} \odot m_t \\
			v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\end{cases}
		\end{equation*}
		\item BFGS
		\begin{equation*}
			\begin{cases} \text{Khởi tạo } H_0 \\
			g_t = \nabla f(w_{t-1}) \\
			p_t = H_{t-1} g_t \\
			w_t = w_{t-1} - \alpha p_t \\
			s_t = w_t - w_{t-1} \\
			y_t = \nabla f(w_t) - \nabla f(w_{t-1}) \\
			H_t = \Bigg(I - \dfrac{s_t y_t^T}{y_t^T s_t} \Bigg)H_{t-1}\Bigg( I - \dfrac{y_t s_t^T}{y_t^T s_t} \Bigg) + \dfrac{s_t s_t^T}{y_t^T s_t}\end{cases}
		\end{equation*}
		\framebreak
		\item DFP
		\begin{equation*}
			\begin{cases} \text{Khởi tạo } H_0 \\
			g_t = \nabla f(w_{t-1}) \\
			p_t = H_{t-1} g_t \\
			w_t = w_{t-1} - \alpha p_t \\
			s_t = w_t - w_{t-1} \\
			y_t = \nabla f(w_t) - \nabla f(w_{t-1}) \\
			H_t = H_{t-1} - \dfrac{H_{t-1}y_t y_t^T H_{t-1}}{y_t^T H_{t-1} y_t} + \dfrac{s_t s_t^T}{y_t^T s_t}\end{cases}
		\end{equation*}
	\end{enumerate}
\end{frame}

\end{document}